---
layout: post
title: 神经网络和深度学习简史（二）
category: Document
tags: ml
date: 2017-11-15 13:11:34
published: true
summary: 本文我们将研究一些在反向传播发展之后取得快速进展的研究，直到90年代后期，我们将在后面看到它们是深度学习的基本基础。[第一部分](http://fooyou.github.io/blog/a-brief-history-of-neural-nets-and-deep-learning-part-1)
image: pirates.svg
comment: true
---

# 第二部分：神经网络的苏醒

## 基于神经网络的计算机视觉

![Yann LeCun 的 LeNet 演示](http://yann.lecun.com/exdb/lenet/gifs/asamples.gif)

随着训练多层神经网络的谜题被揭开，这个话题再一次变得空前热门，罗森布拉特的崇高雄心似乎也将得以实现。直到1989年另一个关键发现被公布，现在仍广为教科书及各大讲座引用：“多层前馈网络是通用逼近器”。从本质上讲并在数学上得以证明：多层可让神经网络在理论上能实现各种功能，当然也包括抑或逻辑（XOR）。

然而，这是数学，你可以在数学中畅想自己拥有无限内存和所需计算能力——反向传播可以让神经网络被用于世界任何角落吗？噢，当然。也是在1989年，Yann LeCunn在AT&T Bell实验室验证了一个反向传播在现实世界中的杰出应用，即「反向传播应用于手写邮编识别（[Backpropagation Applied to Handwritten Zip Code Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)）」。

你或许会认为，让计算机能够正确理解手写数字并没有那么了不起，而且今天看来，这还会显得你太过大惊小怪，但事实上，在这个应用公开发布之前，人类书写混乱，笔画也不连贯，对计算机整齐划一的思维方式构成了巨大挑战。这篇研究使用了美国邮政的大量数据资料，结果证明神经网络完全能够胜任识别任务。更重要的是，这份研究首次强调了超越普通（plain）反向传播 、迈向现代深度学习这一关键转变的实践需求。

> 传统的视觉模式识别工作已经证明，抽取局部特征并且将它们结合起来组成更高级的特征是有优势的。通过迫使隐藏单元结合局部信息来源，很容易将这样的知识搭建成网络。一个事物的本质特征可以出现在输入图片的不同位置。因此，拥有一套特征探测器，可以探测到位于输入环节任何地方的某个具体特征实例，非常明智。既然一个特征的精准定位于分类无关，那么，我们可以在处理过程中适当舍弃一些位置信息。不过，近似的位置信息必须被保留，从而允许下面网络层能够探测到更加高级更加复杂的特征。（Fukushima1980，Mozer,1987）

![一个神经网络工作原理的可视化过程](https://draftin.com/images/35003?token=pRZiRNO5tZB3uHSXV0bjIdzsP2tAUr9jpXUUChNI20Dwk0Y9JOMcGwtmmlgHGVzgJRGhXsr998Ogpxbl3K1Vn_8)


或者，更具体的：神经网络的第一个隐层是卷积层——不同于传统网络层，每个神经元对应的一个图片像素都相应有一个不同的权值（40 * 60 = 2400 个权值），神经元只有很少一部分权值（5 * 5 = 25）以同样的大小应用于图像的一小个完整子空间。所以，比如替换了用四种不同的神经元来学习整个输入图片4个角的45度对角线探测，一个单独的神经元能通过在图片的子空间上学习探测45度对角线，并且照着这样的方法对整张图片进行学习。每层的第一道程序都以相类似的方式进行，但是，接收的是在前一隐藏层找到的「局部」特征位置而不是图片像素值，而且，既然它们正在结合有关日益增大的图片子集的信息，那么，它们也能「看到」其余更大的图片部分。最后，倒数的两个网络层利用了前面卷积抽象出来的更加高级更加明显的特征来判断输入的图像究竟该归类到哪里。这个在1989年的论文里提出的方法继续成为举国采用的支票读取系统的基础，正如LeCun在如下小视频中演示的：

![Convolutional Network Demo from 1993](../img/posts/Convolutional_Network_Demo_from_1993.jpg)

*1993 年卷积网络识别手写数字示例*

这很管用，为什么？原因很直观，如果数学表述上不是那么清楚的话：没有这些约束条件，网络就必须学习同样的简单事情（比如，检测45°角的直线和小圆圈等），要花大把时间学习图像的每一部分。但是，有些约束条件，每一个简单特征只需要一个神经元来学习——而且，由于整体权值大量减少，整个过程完成起来更快。此外，既然这些特征的像素确切位置无关紧要，在计算应用权重时，基本上可以跳过图像相邻子集——**子采样**，现在称为**共享池手段**（a type of pooling）——当应用权值时，进一步减少了训练时间。多加的这两层（卷积层和共享池层）——是**卷积神经网络**（CNNs/ConvNets）和普通旧神经网络的主要区别。

![卷积神经网络的操作过程](https://draftin.com/images/34967?token=cmXwbZkJ53nKUhEFA3zCrtdFDF1cVgfhGFBv1lD8Z7TPCqZpKRwR0Ht-vE-894hZyaWbYxUX8wak0QjMXvNq8P4)

*卷积神经网络（CNN）的操作过程*

那时，卷积的思想被称作「权值共享」，也在1986年Rumelhart、Hinton和Williams关于反向传播的延伸分析中得到了切实讨论。显然，Minsky和Papert在1969年《感知机》中的分析完全可以提出激发这一研究想法的问题。但是，和之前一样，其他人已经独立地对其进行了研究——比如，Kunihiko Fukushima在1980年提出的 Neurocognitron。而且，和之前一样，这一思想从大脑研究汲取了灵感：

> 根据Hubel和Wiesel的层级模型，视觉皮层中的神经网络具有一个层级结构：LGB（外侧膝状体）→样品细胞→复杂细胞→低阶超复杂细胞->高阶超复杂细胞。低阶超复杂细胞和高阶超复杂细胞之间的神经网络具有一个和简单细胞与复杂细胞之间的网络类似的结构。在这种层状结构中，较高级别的细胞通常会有这样的倾向，即对刺激模式的更复杂的特征进行选择性响应，同时也具有一个更大的接收域，而且对刺激模式位置的移动更不敏感。因此，在我们的模型中就引入了类似于层级模型的结构。

LeCun也在贝尔实验室继续支持卷积神经网络，其相应的研究成果也最终在上世纪90年代中期成功应用于支票读取——他的谈话和采访通常都介绍了这一事实：「在上世纪90年代后期，这些系统当中的一个读取了全美大约10%到20%的支票。」


## 神经网络进入无监督期

将死记硬背，完全无趣的支票读取工作自动化，就是机器学习大展拳脚的例子。也许有一个预测性小的应用？ 压缩。即指找到一种更小体量的数据表示模式，并从其可以恢复数据原有的表示形态，通过机器学习找到的压缩方法有可能会超越所有现有的压缩模式。当然，意思是在一些数据中找到一个更小的数据表征，原始数据可以从中加以重构。学会压缩这一方案远胜于常规压缩算法，在这种情况下，学习算法可以找到在常规压缩算法下可能错失的数据特征。而且，这也很容易做到——仅用训练带有一个小隐藏层的神经网络就可以对输入进行输出:

![自编码神经网络](https://draftin.com/images/34875?token=N8kgwOTY2SLYiUmyWgp6q_SUr2lq1VZRCsqjuEcUzhSyxukW8SaukGh2U-PdFABd3WIkZlgtOr9pbVX_kGGUfnM)

### 自编码神经网络

这是一个自编码神经网络，也是一种学习压缩的方法——有效地将数据转换为压缩格式，并且自动返回到本身。我们可以看到，输出层会计算其输出结果。由于隐藏层的输出比输入层少，因此，隐藏层的输出是输入数据的一个压缩表达，可以在输出层进行重建。

![更明确地了解自编码压缩](https://draftin.com/images/34939?token=mIbhFk3rVIyx-Byzt6TXV1hGzMH7_w5sjy5OzeYM0qex33WDiI1PhspANLICVpp53PZyysX8yR9YahhXtBVkV6M)

*更明确地了解自编码压缩*

注意一件奇妙的事情：我们训练所需的唯一东西就是一些输入数据。这与监督式机器学习的要求形成鲜明的对比，监督式机器学习需要的训练集是输入-输出对（**标记数据**），来近似地生成能从这些输入得到对应输出的函数。确实，自编码器并不是一种监督式学习；它们实际上是一种**非监督式学习**，只需要一组输入数据（**未标记的数据**），目的是找到这些数据中某些隐藏的结构。换句话说，非监督式学习对函数的近似程度不如它从输入数据中生成另一个有用的表征那么多。这样一来，这个表征比原始数据能重构的表征更小，但它也能被用来寻找相似的数据组（**聚类**）或者**潜在变量**（latent variables）的其他推论（某些从数据看来已知存在但数值未知的方面）。

![聚类，一种很常用的非监督式学习应用](https://draftin.com/images/34886?token=iEm6_7c5iGDJPTSNZ-amHkQyb3_f4G3657WBTHcyJJL87dQx8xTMxiJES68Mj0YhbbLx5YWkkaOR4QDkJHiG3-0)

*聚类，一种很常用的非监督式学习应用*


在反向传播算法发现之前和之后，神经网络都还有其他的非监督式应用，最著名的是自组织映射神经网络（SOM，Self Organizing Maps）和自适应共振理论（ART，Adapative Resonance Theory）。SOM能生成低维度的数据表征，便于可视化，而ART能够在不被告知正确分类的情况下，学习对任意输入数据进行分类。如果你想一想就会发现，从未标记数据中能学到很多东西是符合直觉的。假设你有一个数据集，其中有一堆手写数字的数据集，并没有标记每张图片对应着哪个数字。那么，如果一张图片上有数据集中的某个数字，那它看起来与其他大多数拥有同样数字的图片很相似，所以，尽管计算机可能并不知道这些图片对应着哪个数字，但它应该能够发现它们都对应着同一个数字。这样，**模式识别**就是大多数机器学习要解决的任务，也有可能是人脑强大能力的基础。但是，让我们不要偏离我们的深度学习之旅，回到自编码器上。


![自组织映射神经网络](https://draftin.com/images/34887?token=rWIBfCstMS8Y3OsonIyWAOPNHnc9NZIGHs5JesWlo01UCpYKKcMbhJOCj-AvZuDS8VeENeNo35Z1BxQhkOexbHM)

*自组织映射神经网络：将输入的一个大向量映射到一个神经输出的网格中，在其中，每个输出都是一个聚类。相邻的神经元表示同样的聚类。*


正如权重共享一样，关于自编码器最早的讨论是在前面提到过的1986年的反向传播分析中所进行。有了权重共享，它在接下来几年中的更多研究中重新浮出了水面，包括Hinton自己。这篇论文，有一个有趣的标题：《[自编码器，最小描述长度和亥姆霍兹自由能](http://www.cs.toronto.edu/~fritz/absps/cvq.pdf)》（Autoencoders, Minimum Description Length, and Helmholts Free Energy），提出「最自然的非监督式学习方法就是使用一个定义概率分布而不是可观测向量的模型」，并使用一个神经网络来学习这种模型。所以，还有一件你能用神经网络来做的奇妙事：对概率分布进行近似。


## 神经网络迎来信念网络

事实上，在成为1986年讨论反向传播学习算法这篇有重大影响力论文的合作者之前，Hinton在研究一种神经网络方法，可以学习1985年《[A Learning Algorithm for Boltzmann Machines](http://www.cs.toronto.edu/~fritz/absps/cogscibm.pdf)》中的概率分布。玻尔兹曼机就是类似神经网络的网络，并有着和感知器（Perceptrons）非常相似的单元，但该机器并不是根据输入和权重来计算输出，在给定相连单元值和权重的情况下，网络中的每个单元都能计算出自身概率，取得值为1或0。因此，这些单元都是随机的——它们依循的是概率分布而非一种已知的决定性方式。玻尔兹曼部分和概率分布有关，它需要考虑系统中粒子的状态，这些状态本身基于粒子的能量和系统本身的热力学温度。这一分布不仅决定了玻尔兹曼机器的数学方法，也决定了其推理方法——网络中的单元本身拥有能量和状况，学习是由最小化系统能量和热力学直接刺激完成的。虽然不太直观，但这种基于能量的推理演绎实际上恰是一种**基于能量的模型**的例子，并能够适用于**基于能量的学习**理论框架，而很多学习算法都能用这样的框架进行表述。

> *附：基于能量模型的更多内容*

> 对于一堆学习方法而言，有一个共同的理论框架并不令人意外，归根到底，所有的学习都归结为优化，引用上面引用内容：

> “训练一个 EBM 包括找到一个能产生最好的 Y 和 任何 X 的能量函数…… EBM 的架构师参数化能量函数 E(W, Y, X) 的内部结构……这种质量检测成为损失函数（即函数的函数）并表示为 L(E, S)。…… 为找到最好的能量函数[]，我们需要一种方法来评估任何特定的能量函数的质量，完全基于两个要素：训练集和我们对任务的先前知识。简单起见，我们经常用 L(W, S)来表示它，并简单称之为损失函数。学习问题很简单，就是找到最小化损失的 W。”

> 因此基于能量的模型的关键是认识到所有这些算法从本质上讲是用不同的方法来优化这一对函数：能量函数 E 和损失函数 L，通过找到一组好的值到一堆变量，表示为 W，输入的数据为 X，输出为 Y。这对于一个框架来说确实是一个非常广泛的定义，但是仍然很好地封装了很多算法从根本上再做的事。


![信念网络或者说贝叶斯网络](https://draftin.com/images/34928?token=uZt9tR3PMJ7XcI0pscNEF0hgpiGBmAWdxlT-mXi88-6jI1VKnv5eRXDeX2soiwQ2MJJuq1QeKvSOb1JiviyiZl8)

*一个简单的信念网络/贝叶斯网络*

一个简单的信念，或者说贝叶斯网络——玻尔兹曼机器基本上就是如此，但有着非直接/对称联系和可训练式权重，能够学习特定模式下的概率。

回到玻尔兹曼机器。当这样的单元一起置于网络中，就形成了一张图表，而数据图形模型也是如此。本质上，它们能够做到一些非常类似普通神经网络的事：某些隐藏单元在给定某些代表可见变量的可见单元的已知值（输入——图像像素，文本字符等）后，计算某些隐藏变量的概率（输出——数据分类或数据特征）。以给数字图像分类为例，隐藏变量就是实际的数字值，可见变量是图像的像素；给定数字图像「1」作为输入，可见单元的值就可知，隐藏单元给图像代表「1」的概率进行建模，而这应该会有较高的输出概率。

玻尔兹曼机器实例。每一行都有相关的权重，就像神经网络一样。注意，这里没有分层——所有事都可能跟所有事相关联。我们会在后文讨论这样一种变异的神经网络












参考：

> http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-2/

> https://mp.weixin.qq.com/s/iXtyLK8YHxQT09JufNF4EA##
